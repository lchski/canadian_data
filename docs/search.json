[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analyzing Candian demographic and housing data using R",
    "section": "",
    "text": "This book is intended for people interested in learning how to access, process, analyze, and visualize Canadian demographic, economic, and housing data using R. The target audience we have in mind ranges from interested individuals interested in understanding their environment through data, community activists and community groups interested in introducing data-based approached into their work, journalists who want to report on data in their stories or aim to incorporate their own descriptive data analysis, non-profits or people involved in policy who are looking for data-based answers to their questions.\nThe most important prerequisite is a keen interest in using data to help understand how housing and demographics shape cities and rural areas in Canada, and a willingness to learn. Prior knowledge of R is not necessary, but may be beneficial.\nCanada has high quality demographic, economic and housing data. While significant data gaps exist, the available data often remains under-utilized in policy and planning analyses. Moreover, many analyses that do come out go quickly out of date and can’t easily be updated because they rely on non-reproducible and non-adaptable workflows.\nIn this book we will maintain a strong emphasis on reproducible and adaptable work flows to ensure the analysis is transparent, can easily be updated as new data becomes available, and can be tweaked or adapted to address related questions.\n\n\nThis book will take a project based approach to to teach through examples, with one project per section. Each project will be loosely broken up into four parts.\n\nFormulating the question. What is the question we are interested in? Asking a clear question will help focus our efforts and ensure that we don’t aimlessly trawl through data.\nIdentifying possible data sources. Here we try to identify data sources that can speak to our question. We will also take the time to read up on definitions and background concepts to better understand the data and prepare us for data analysis.\nData acquisition. In this step we will import the data into our current working session. This could be as simple as an API call, or more complicated like scraping we table from the web, or involve even more complex techniques to acquire the data.\nData preparation. In this step we will reshape and filter the data to prepare it for analysis.\nAnalysis. This step could be as simple as computing percentages or even doing nothing, is the quantities we are interested in already come with the dataset, if our question can be answered by a simple descriptive analysis. In other cases, when our question is more complex, this step may be much more involved. The book will try to slowly build up analysis skills along the way, with increasing complexity of questions and required analysis.\nVisualization. The final step in the analyusis process is to visualize and communicate the results. In some cases this can be done via a table or a couple of paragraphs of text explaining the results, but in most cases it is useful to produce graphs or maps or even interactive visualizations to effectively communicate the results.\nInterpretation. What’s left to wrap this up is to interpret the results. How does this answer our question, where does it fall short. What does this mean in the real-world context? What new questions emerge from this?\n\nWhile we won’t always follow this step by step process to the letter, it will be our guiding principle throughout the book. Sometimes things won’t go so clean, where after the visualization step we notice that something looks off or is unexpected, and we may jump back up a couple of steps and add more data and redo parts of the analysis to better understand our data and how it speaks to our initial questions. We might even come to understand that our initial question was not helpful or was ill-posed, and we will come back to refine it.\n\n\n\nBy taking this approach we have several goals in mind:\n\nStay motivated by using real world Canada-focused and (hopefully) interesting examples.\nTeach basic data literacy, appreciate definitions and quirks in the data.\nExpose the world of Canadian data and make it more accessible.\nLearn how data can be interpreted in different ways, and data and analysis is not necessarily “neutral”.\nLearn how to effectively communicate results.\nLearn how to adapt and leverage off of previous work to answer new questions.\nLearn how to reproduce and critique data analysis.\nBuild a community around Canadian data, where people interested in similar questions, or people using the same data, can learn from each other.\nRaise the level of understanding of Canadian data and data analysis so we are better equipped to tackle the problems Canada faces.\n\nThis is setting a very high goal for this book, and we are not sure we can achieve all of this. But we will try our best to be accessible and interesting as possible.\n\n\n\nMost people reading this book will not have used R before, or only used it peripherally, maybe during a college course many years in the past. Instead, readers may be familiar with working through housing and demographic data in Excel or similar tools. Or making maps in QGIS or similar tools when dealing with spatial data. And the type of analysis outlined above that this book will teach can in general terms be accomplished using these tools.\nBut where tools like spreadsheets and desktop GIS fall short is in another important focus of this book: transparency, reproducibility, and adaptability.\nAn analysis in a spreadsheet or desktop GIS typically involves a lot of manual steps, the work is not reproducible without repeating these steps. We can’t easily inspect how the result was derived, the analysis lacks transparency. When we just compute a ratio or percentage this may not be so bad, but trying to understand how a more complex analysis was done in a spreadsheet easily turns into a nightmare. Analysis that involves a lot of manual steps is not auditable without putting in the work to repeat those manual steps.\nBut why does this matter? It’s always been this way, some experts produce analysis and produce a glossy paper to present the results. One can argue if this was an adequate modus operandi in the past, but we feel strongly that it’s not in today’s world. The lines between experts and non-experts has become blurred, and the value we place on lived experience has increased relative to more formal expertise. We argue this places different demands on policy-relevant analysis, it needs to be open and transparent, in principle anyone should be able to understand how the analysis was done and the conclusions were reached. That’s where reproducibility and transparency come in. And it also requires bringing up data analysis skills in the broader population, so that the ability to reproduce and critique an analysis in principle can be realized in practice.\nThe remaining reason for using R, adaptability, has also become increasingly important. The amount of data available to us has increased tremendously, but our collective ability to analyse data and extract information has not kept up. Doing analysis in R allows us to efficiently reuse previous analysis to perform a similar one. Or to build on previous analysis to deepen it. Which turbocharges our ability to do analysis, covering more ground and going deeper.\nR is not the only framework to do this in, there are other options like python or julia. But we believe that R is best suited for people transitioning into this space, and we can rely on an existing ecosystem of packages to access and process Canadian data. People already proficient in python will have no problem translating what we do into their preferred framework, or dynamically switch back and forth between R, python or whatever other tools they prefer as needed and convenient.\n\n\n\nWhich brings us to our most ambitious goal, to help create a community around Canadian data analysis. When analysis is transparent, reproducible and adaptable people can piggy-back of each other’s work, reusing parts of analysis others have done and building and improving upon it. Or Critiquing and correcting analysis, or taking it toward a different direction. A community that grows in their understanding of data, and a community using a shared set of tools to access and process Canadian data, enabling discussions to move forward instead of in circles. A community that builds up expertise from the bottom up.\nThe book tries to address both of these requirements for building a Canadian data community, a principled approach to data and data analysis, while introducing R as a common framework to work in hoping that the reader will come away with\n\nbetter data literacy skills to understand and critique data analysis,\ntechnical skills to reproduce and perform their own data analysis, and\na common tool set for acquiring, processing and analyzing Canadian data that facilitates collaborative practices."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "In this section we give a taste of what’s to come. Some of the concepts introduced in the preface may be too abstract to picture for people just starting out in this space. People probably grasp the importance of having a principled approach to data analysis, from formulating a question all the way to sharing results. But why so much emphasis on reproducibility and adaptability? And do we really need to learn a new framework like R for this?\nThis is best understood by walking through a simple example of what analysis of Canadian data in R, and a Canadian data community might look like. We won’t explain all steps in full detail here, this is to serve to illustrate the concepts talked above in the preface and give the reader a taste of what’s to come.\nIf you don’t understand all the code now, don’t worry, that’s part of the point of this book. We will work out and explain these examples in detail in the first chapter of the book. What’s important right now is to illustrate the principle of reproducible and adaptable code, and how this can function to foster a community of Canadian data analysis. And to note how little code is needed to make this work."
  },
  {
    "objectID": "intro.html#a-hypothetical-example",
    "href": "intro.html#a-hypothetical-example",
    "title": "1  Introduction",
    "section": "1.1 A hypothetical example",
    "text": "1.1 A hypothetical example\nImagine Amy, a Toronto-based social services worker looking to pilot a community intervention targeted at children in low income. She is in the process of putting together a proposal describing her intervention and is trying to locate a good neighbourhood for her pilot and make a compelling case to possible funders.\nAmy knows that census data has a good geographic breakdown of children in poverty, but the latest available data is from 2016, using 2015 income data. CRA tax data is available up to 2019, but also has information on families in low income, but nothing directly on children in the standard release tables at fine geographies. As a first step she settles on census data, with the goal to re-run the analysis once the 2021 data comes out later in the year.\nShe refers to the Census Dictionary to understand the various low income measures, and uses CensusMapper’s interactive map that allows to explore these concepts. She would have liked to use the Market Based Measure, but due to data availability she settles for LICO-AT.\nShe sets up a new Notebook and loads in the R libraries that she will need for this, ggplot2for graphing and cancensus for ingesting the data.\n\nlibrary(cancensus)\nlibrary(ggplot2)\n\nNext she pull in the data. the CensusMapper API GUI tool helps her locate the StatCan geographic identifier for Toronto, (3520005), and the internal CensusMapper vector for the percentage of children in LICO-AT (v_CA16_2573).\n\nlico_yyz <- get_census(\"CA16\",regions=list(CSD=\"3520005\"), vectors=c(lico=\"v_CA16_2573\"),\n                        level=\"CT\",geo_format=\"sf\")\n\nHere Amy specified that she wants data for the 2016 Canadian census (“CA16”), the region and vectors, at the census tract (“CT”) level, with geographies as well as the low income data.\nNow that she has the data at her finger times her first step is to make a map. For that she needs to tell ggplot is what variable to use as fill colour, and maybe give it a nicer colour scale and some labels to explain what the map is about.\n\nggplot(lico_yyz, aes(fill=(lico/100))) +\n  geom_sf() +\n  scale_fill_viridis_c(labels=scales::percent) +\n  coord_sf(datum=NA) +\n  labs(title=\"Children in low income (LICO-AT)\",\n       fill=\"Share\",\n       caption=\"StatCan census 2016\")\n\n\n\n\nBased on this she locates a couple of good candidate neighbourhoods for her pilot and sends the map in a email to her colleague Peter to get input on which neighbourhood might be best suited.\nPeter has some good feedback for Amy, but also gets an idea to try and set up something similar in Vancouver. Peter asks Amy if she can share the code, and Amy sends along the above code snippets. Peter looks up the geographic identifier for Vancouver and subs that in instead of Toronto’s.\n\nlico_yvr <- get_census(\"CA16\",regions=list(CSD=\"5915022\"), vectors=c(lico=\"v_CA16_2573\"),\n                        level=\"CT\",geo_format=\"sf\")\n\nggplot(lico_yvr, aes(fill=(lico/100))) +\n  geom_sf() +\n  scale_fill_viridis_c(labels=scales::percent) +\n  coord_sf(datum=NA) +\n  labs(title=\"Children in low income (LICO-AT)\",\n       fill=\"Share\",\n       caption=\"StatCan census 2016\")\n\n\n\n\nEasy peasy, thanks to Amy’s previous work. Peter takes the map to his friend Yuko and asks her for advice where a community-based intervention for low-income children might make sense in Calgary. Yuko asks for the code from Peter to take a closer look herself.\nYuko is interested in a finer geographic breakdown, so she swaps our the geographic level from census tracts to dissemination areas.\n\nlico_yvr_da <- get_census(\"CA16\",regions=list(CSD=\"5915022\"), vectors=c(lico=\"v_CA16_2573\"),\n                        level=\"DA\",geo_format=\"sf\")\n\nggplot(lico_yvr_da, aes(fill=(lico/100))) +\n  geom_sf(size=0.1) +\n  scale_fill_viridis_c(labels=scales::percent) +\n  coord_sf(datum=NA) +\n  labs(title=\"Children in low income (LICO-AT)\",\n       fill=\"Share\",\n       caption=\"StatCan census 2016\")\n\n\n\n\nBut then Yuko pauses to think that maybe looking at share of the low income population is not the right metric. She decides to query the number of children in low income (vector “v_CA16_2558”) and prepare the data for a dot-density map.\n\nlibrary(dotdensity)\nlibrary(dplyr)\n\nlico_dots_yvr <- get_census(\"CA16\",regions=list(CSD=\"5915022\"),geo_format=\"sf\",\n                            vectors=c(lico=\"v_CA16_2558\"), level=\"DA\") %>%\n  compute_dots(\"lico\")\nyvr_city <- get_census(\"CA16\",regions=list(CSD=\"5915022\"),geo_format=\"sf\")\n\nggplot(lico_dots_yvr) +\n  geom_sf(data = yvr_city) +\n  geom_sf(size=0.25,colour=\"brown\",alpha=0.1) +\n  coord_sf(datum=NA) +\n  labs(title=\"Children in low income (LICO-AT)\",\n       fill=\"Share\",\n       caption=\"StatCan census 2016\")\n\n\n\n\nThat paints a somewhat different picture, and Yuko feels this is much better suited to pinpoint where to best stage a community intervention. She lets Peter and Amy know and emails them her modifications to the code.\nMeanwhile, Yuko’s Vancouver friend Stephanie is looking specifically at children below the age of 6 in low income, and wants to understand how the geographic distribution of low income children has changed over time. Comparing census data through time can be tricky because census geographies change, but this problem has been completely solved via the tongfen R package. Looking at Yuko’s work she thinks it might be best to look at both, the change in share of children in low income as well as the change in absolute number.\n\nlibrary(tongfen)\nmeta <- meta_for_ca_census_vectors(c(total_2006=\"v_CA06_1982\",lico_share_2006=\"v_CA06_1984\",\n                                     lico_2016=\"v_CA16_2561\",lico_share_2016=\"v_CA16_2576\"))\n\nlico_data <- get_tongfen_ca_census(regions=list(CSD=\"5915022\"),meta,level=\"CT\") %>%\n  mutate(lico_2006=total_2006*lico_share_2006/100) %>%\n  mutate(`Absolute change`=lico_2016-lico_2006,\n         `Percentage point change`=lico_share_2016-lico_share_2006)\n\nArmed with this data Stephanie can plot the absolute and percentage point change in children below 6 in low income.\n\nggplot(lico_data,aes(fill=`Absolute change`)) +\n  geom_sf() +\n  scale_fill_gradient2() +\n  coord_sf(datum=NA) +\n  labs(title=\"Change in number of children under 6 in low income\",\n       caption=\"StatCan Census 2006, 2016\")\n\n\n\n\n\nggplot(lico_data,aes(fill=`Percentage point change`/100)) +\n  geom_sf() +\n  scale_fill_gradient2(labels=scales::percent) +\n  coord_sf(datum=NA) +\n  labs(title=\"Change in share of children under 6 in low income\",\n       fill=\"Percentage\\npoint change\",\n       caption=\"StatCan Census 2006, 2016\")\n\n\n\n\nStephanie shares her results with Amy in Toronto in case there are components of Amy’s pilot specifically targeting children below 6 in low income.\nMeanwhile Amy has been trying to understand more broadly how the share of low income children has evolved since the 2016 census (using 2015 income data) at the metropolitan level over longer time spans, so she looks through the StatCan socioeconomic tables and settles on table 11-10-0135, which also allows her to compare various low income concepts.\n\nlibrary(cansim)\nmbm_timeline <- get_cansim(\"11-10-0135\") %>%\n  filter(`Persons in low income`==\"Persons under 18 years\",\n         GEO==\"Toronto, Ontario\",\n         Statistics==\"Percentage of persons in low income\")\n\nggplot(mbm_timeline,aes(x=Date,y=val_norm,colour=`Low income lines`)) +\n  geom_point(shape=21) +\n  geom_line() +\n  scale_y_continuous(labels=scales::percent) +\n  labs(title=\"Children in low income in Metro Toronto\",\n       y=\"Share of children in low income\",\n       x=NULL,\n       caption=\"StatCan Table 11-10-0135\")\n\n\n\n\nShe notes that there has been a substantial overall drop in children in low income since 2015 across all measures, which is excellent news. She considers pushing off her pilot project until after the 2021 census data comes out to first understand if the geographic patterns have changed."
  },
  {
    "objectID": "intro.html#what-you-will-learn-in-this-book",
    "href": "intro.html#what-you-will-learn-in-this-book",
    "title": "1  Introduction",
    "section": "1.2 What you will learn in this book",
    "text": "1.2 What you will learn in this book\nLooking at R code for the first time can be intimidating. If the code looks opaque right now, there is no need to worry. It will be explained in detail in the first chapter and is very much part of the rationale for writing this book. If decisions around what low income metric to pick, or why tongfen is needed to compare census data through time are not clear, again, that will be explained in this book in detail and expanding understanding of data and data analysis is the other big rationale for this book.\nReaders will learn how to reproduce analysis, how to critique analysis, and adapt it for their own purposes. And readers will learn how to conduct their own analysis in the Canadian context, based on questions and use cases relevant to them.\nHopefully the above hypothetical scenario have explained how the adaptability of the R code has made life much easier for several of the subsequent analysis steps, and how little code was needed to gain some insights and communicate results."
  },
  {
    "objectID": "intro_to_r.html",
    "href": "intro_to_r.html",
    "title": "Getting started with R and RStudio",
    "section": "",
    "text": "Statistics Canada produces a lot of high quality demographic and economic data for Canada. CMHC complements this with housing data, and municipalities across Canada often provide relevant data through their Open Data portals."
  },
  {
    "objectID": "intro_to_r.html#r-and-rstudio",
    "href": "intro_to_r.html#r-and-rstudio",
    "title": "Getting started with R and RStudio",
    "section": "R and RStudio",
    "text": "R and RStudio\nWe will be working in R and the RStudio IDE, although using a different editor like Visual Studio Code works just as well, especially if you are already familiar with it. Within R we will be operating within the tidyverse framework, a group of R packages that work well together and allow for intuitive operations on data via pipes.\nWhile an introduction to R is part of the goal of this book, an we will slowly build up skills as we go, we not give a systematic introduction but rather build up skills slowly as we work on concrete examples. It may be beneficial to supplement this with a more principled introduction to R and the tidyverse.\nDuring the course of this book we will make heavy use of several R packages to facilitate data access, we will introduce them in this chapter."
  },
  {
    "objectID": "intro_cansim.html",
    "href": "intro_cansim.html",
    "title": "2  Introduction to cansim",
    "section": "",
    "text": "The cansim R package interfaces with the StatCan NDM that replaces the former CANSIM tables. It can be queried for\n\nwhole tables\nspecific vectors\ndata discovery searching through tables\n\nIt encodes the metadata and allows to work with the internal hierarchical structure of the fields.\nLarger tables can also be imported into a local SQLite database for reuse across sessions without the need to re-download the data, and better performance when subsetting the data."
  },
  {
    "objectID": "intro_cancensus.html",
    "href": "intro_cancensus.html",
    "title": "3  Introduction to cancensus",
    "section": "",
    "text": "The cancensus R package interfaces with the CensusMapper API server. It can be queried for\n\ncensus geographies\ncensus data\nhierarchical metadata of census variables\nsome non-census data that comes on census geographies, e.g. T1FF taxfiler data\n\nA slight complication, the cancensus packageneeds an API key. You can sign up for one on CensusMapper."
  },
  {
    "objectID": "basic.html",
    "href": "basic.html",
    "title": "Basic descriptive analysis",
    "section": "",
    "text": "The accompanying analysis won’t be very involved, sometimes we will compute percentages or make other simple data manipulations, but generally the analysis will be quite straight-forward. We will focus on how to find data sources that can inform on our question, how to get the data, and how to present and interpret it."
  },
  {
    "objectID": "basic_bc_pop.html",
    "href": "basic_bc_pop.html",
    "title": "4  BC population growth",
    "section": "",
    "text": "Let’s first try and understand what the headline really means. B.C. “welcoming” people refers to people moving to the province from elsewhere, either from other provinces or internationally. So this is referring to gross in-migration. But reading the text of the press release it immediately pivots to a different concept, saying that “B.C.’s net migration reached 100,797 people in 2021”. It helpfully explains that net migration is the difference between people moving here and people moving away. Which is quite different from the number of people B.C. “welcomed” that year, or the number of people “moving to the province in 2021” as implied by the title and the first sentence of the press release.\nSo here comes the first difficulty, the press release is contradicting itself by mixing two concepts. That leads us to formulate a fairly broad question that should help clear this up.\n\n4.0.1 Question\nHow many people has B.C. welcomed, net and gross, how has that changed over the last 6 decades, and how should this be interpreted?\n\n\n4.0.2 Data sources\nTo start, let’s figure out where that data point comes from.\nThe press release references StatCan as the source, let’s search through the StatCan tables. Google usually works reasonably well, but we can also search programmatically. We are looking for migration estimates from the quarterly demographic estimates to get the most up-to-data population estimates from StatCan. For results we just need the first two columns, that table number and the title.\n\nlibrary(tidyverse)\nlibrary(cansim)\n\nsearch_cansim_cubes(\"migration\") %>% \n  filter(grepl(\"quarterly\",cubeTitleEn)) %>%\n  arrange(desc(cubeEndDate)) %>% \n  select(1:2)\n\n# A tibble: 2 × 2\n  cansim_table_number cubeTitleEn                                               \n  <chr>               <chr>                                                     \n1 17-10-0020          Estimates of the components of interprovincial migration,…\n2 17-10-0040          Estimates of the components of international migration, q…\n\n\nIt looks like Table 17-10-0020 and 17-10-0040 are what we are looking for. Let’s load in the data and inspect the first couple of rows for BC.\n\n\n4.0.3 Data acquisition\n\ninterprovincial <- get_cansim(\"17-10-0020\")\n\nAccessing CANSIM NDM product 17-10-0020 from Statistics Canada\n\n\nParsing data\n\ninternational <- get_cansim(\"17-10-0040\")\n\nAccessing CANSIM NDM product 17-10-0040 from Statistics Canada\nParsing data\n\ninterprovincial %>%\n  filter(GEO==\"British Columbia\") %>%\n  select(GEO,Date,`Interprovincial migration`,val_norm) %>%\n  tail()\n\n# A tibble: 6 × 4\n  GEO              Date       `Interprovincial migration` val_norm\n  <chr>            <date>     <fct>                          <dbl>\n1 British Columbia 2021-04-01 In-migrants                    31926\n2 British Columbia 2021-04-01 Out-migrants                   16636\n3 British Columbia 2021-07-01 In-migrants                    19352\n4 British Columbia 2021-07-01 Out-migrants                   13575\n5 British Columbia 2021-10-01 In-migrants                    12120\n6 British Columbia 2021-10-01 Out-migrants                    8787\n\n\nFor inter-provincial migration we get in and out migration counts for every quarter. Let’s also inspect the international migration data.\n\ninternational %>%\n  filter(GEO==\"British Columbia\") %>%\n  select(GEO,Date,`Components of population growth`,val_norm) %>%\n  tail()\n\n# A tibble: 6 × 4\n  GEO              Date       `Components of population growth` val_norm\n  <chr>            <date>     <fct>                                <dbl>\n1 British Columbia 2021-07-01 Net non-permanent residents           9035\n2 British Columbia 2021-10-01 Immigrants                           23281\n3 British Columbia 2021-10-01 Emigrants                             3759\n4 British Columbia 2021-10-01 Returning emigrants                    581\n5 British Columbia 2021-10-01 Net temporary emigrants                779\n6 British Columbia 2021-10-01 Net non-permanent residents          -6718\n\n\nHere we get immigrants, emigrants, returning emigrants, but for temporary emigrants and non-permanent residents we only get net change. That puts a bit of a damper on our ambition to look at gross migration, for those last two categories net is all we have.\n\n\n4.0.4 Data preparation\nNext we got to wrangle this data into a useful format. We are interested in all of these components, so we need to join these two data series together. We will retain the GeoUID, GEO, Components of population growth, Date and val_norm columns, which requires some renaming and then defining factor levels so that they stack nicely later in our plots. We also flip the sign on out-migrants and emigrants, as these are out-flows. To make sure those two time series start at the same time we cut it off appropriately.\nThe press release talked about annual change, so we do a rolling sum over 4 quarters, right-aligning the data so it’s for the period of the preceding year.\n\nmigration_data <- bind_rows(\n  interprovincial %>% \n    select(GeoUID,GEO,Date,\n           `Components of population growth`=`Interprovincial migration`,val_norm) %>%\n    mutate(`Components of population growth`=\n             paste0(\"Interprovincial \",tolower(`Components of population growth`))),\n  international %>% \n    select(GeoUID,GEO,Date,`Components of population growth`,val_norm)\n) %>%\n  mutate(`Components of population growth`=\n           factor(`Components of population growth`,\n                  levels=c(\"Interprovincial out-migrants\",\n                           \"Emigrants\",\n                           \"Interprovincial in-migrants\",\n                           \"Immigrants\",\n                           \"Returning emigrants\",\n                           \"Net temporary emigrants\",\n                           \"Net non-permanent residents\"))) %>%\n  mutate(value=ifelse(`Components of population growth` %in% \n                        c(\"Interprovincial out-migrants\",\"Emigrants\"),\n                      -val_norm,val_norm)) %>%\n  filter(Date>=pmax(min(interprovincial$Date),min(international$Date))) %>%\n  group_by(GeoUID,`Components of population growth`) %>%\n  arrange(Date) %>%\n  mutate(annual=zoo::rollsum(value,k=4,na.pad = TRUE,align = \"right\")) %>%\n  filter(!is.na(annual)) %>%\n  ungroup()\n\nWe will also need net migration stats, so let’s compute these by summing of the components,\n\nnet_migration <- migration_data %>% \n  group_by(Date,GEO,GeoUID) %>%\n  summarize(value=sum(value),annual=sum(annual),.groups=\"drop\") %>%\n  mutate(`Components of population growth`=\"Net migration\")\n\n\n\n4.0.5 Analysis and visualization\nTime to make a graph.\n\nggplot(migration_data %>% filter(GEO==\"British Columbia\")) +\n  geom_area(aes(x=Date,y=annual,fill=fct_rev(`Components of population growth`)),\n           stat=\"identity\") +\n  scale_y_continuous(labels=scales::comma) +\n  geom_line(data=net_migration %>% filter(GEO==\"British Columbia\"),\n            aes(x=Date,y=annual)) +\n  MetBrewer::scale_fill_met_d(\"Archambault\") +\n  labs(title=\"BC Year over year components of population change\",\n       y=\"Year over year change\",x=NULL,fill=\"Components of population change\",\n       caption=\"StatCan Tables 17-10-0020, 17-10-0040\")\n\n\n\n\nThis shows us that the press report was most likely talking did not mean to talk about number of people B.C. has “welcomed” or that “moved to the province” but instead the difference between the number of people it welcomed and the number of people it bid farewell.\nAnd the net migration is indeed at record levels. At least in absolute terms. But B.C. now is very different from B.C. in the 60s at the start of this time series. How can we compare net migration over time in a more meaningful way? Normalizing by population is a good option here. Let’s grab the data and take a look how B.C. population has changed.\n\npop_data <- get_cansim(\"17-10-0009\") %>%\n  select(GEO,Date,Population=val_norm)\n\nAccessing CANSIM NDM product 17-10-0009 from Statistics Canada\n\n\nParsing data\n\npop_data %>% \n  filter(GEO==\"British Columbia\") %>%\n  ggplot(aes(x=Date,y=Population)) +\n  geom_line() +\n  scale_y_continuous(labels=scales::comma) +\n  labs(title=\"Population estimates for British Columbia\",\n       y=\"Number of people\",\n       x=NULL,\n       caption=\"StatCan Table 17-10-0009\")\n\n\n\n\nIndeed, the trend is quite strong. Let’s fold that in and normalize by population.\n\nmigration_data %>% \n  left_join(pop_data, by=c(\"GEO\",\"Date\")) %>%\n  filter(GEO==\"British Columbia\") %>%\n  ggplot() +\n  geom_area(aes(x=Date,y=annual/Population,fill=fct_rev(`Components of population growth`)),\n           stat=\"identity\") +\n  scale_y_continuous(labels=scales::comma) +\n  geom_line(data=net_migration %>% \n              left_join(pop_data, by=c(\"GEO\",\"Date\")) %>%\n              filter(GEO==\"British Columbia\"),\n            aes(x=Date,y=annual/Population)) +\n  MetBrewer::scale_fill_met_d(\"Archambault\") +\n  labs(title=\"BC Year over year components of population change\",\n       y=\"Year over year change per population\",x=NULL,fill=\"Components of population change\",\n       caption=\"StatCan Tables 17-10-0020, 17-10-0040, 17-10-0009\")\n\n\n\n\nHere the picture looks a little different. Net migration per capita is at its highest since the 90s, but the past 60 years there were several periods where it was larger.\n\n\n4.0.6 Interpretation\nThis answers our question, the latest annual net migration edges over the 100,000 people mark, and in absolute terms this is the highest it’s been over at least 60 years. But what can we learn from that?\nB.C. 60 years ago is very different from B.C. today. To account for that we can normalize by population, and the relative net migration has been higher at several times during the past 60 years, most recently in the 90s.\nWe also note that big dip in net-migration during COVID-19. It is not clear if the current heights are a bounce-back to make up for the comparatively low net in-migration during the pandemic, or if it is simply reverting back to the increasing trend we have seen over the past 10 years."
  }
]